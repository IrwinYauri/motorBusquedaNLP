# MOTOR DE B√öSQUEDA üöÄ

_Trabajo final EDA - Maestr√≠a en CC - Unsa_

_Alum. Irwin L. Yauri Orihuela_


## 1.	INTRODUCCI√ìN üöÄ

_Elaborar un motor de b√∫squeda que permita recuperar informaci√≥n del DataSet de **Wikipedia**._

### 1.1.	DataSet Wikipedia üìã

_Todo el contenido de Wikipedia se encuentra disponible bajo un cierto esquema de licenciamiento que permite que sea copiado, modificado y redistribuido con pocas restricciones. Se encuentra en formato  ZIM (en ingl√©s, Zeno IMprove, en espa√±ol, Zeno MEjorado). El formato permite la compresi√≥n de art√≠culos, cuenta con un √≠ndice de b√∫squeda de texto completo y categor√≠as. El original xml Wikipedia en enero de 2012 contaba con unos 3,8 millones de art√≠culos con im√°genes, ten√≠a un tama√±o de 7,5 GiB mientras que el archivo ZIM equivalente era de 9.7 GiB (aproximadamente un 30% mayor). El formato de archivo abierto openZIM ofrece apoyo a un lector de ZIM de c√≥digo abierto._

```
https://es.wikipedia.org/wiki/Wikipedia:Descargas
```

### 1.2.	Motor de b√∫squeda üìã

_Un motor de b√∫squeda es una herramienta que permite a los usuarios localizar informaci√≥n de manera r√°pida y sencilla. Cada motor de b√∫squeda utiliza diferentes f√≥rmulas matem√°ticas complejas para generar resultados de b√∫squeda. Los algoritmos de los motores de b√∫squeda toman los elementos clave de una p√°gina web, incluido el t√≠tulo de la p√°gina, el contenido y la densidad de palabras clave. Los motores de b√∫squeda solo "ven" el texto en las p√°ginas web y utilizan la estructura HTML subyacente para determinar la relevancia. Las fotos grandes o la animaci√≥n Flash din√°mica no significan nada para los motores de b√∫squeda, pero el texto real de las p√°ginas s√≠._

### 1.3.	Procesamiento del Lenguaje Natural (NLP) üìã

_Es una disciplina con una larga trayectoria. Nace en la d√©cada de 1960, como un sub√°rea de la Inteligencia Artificial y la Ling√º√≠stica, con el objeto de estudiar los problemas derivados de la generaci√≥n y comprensi√≥n autom√°tica del lenguaje natural.

En sus or√≠genes, sus m√©todos tuvieron gran aceptaci√≥n y √©xito, no obstante, cuando sus aplicaciones fueron llevadas a la pr√°ctica, en entornos no controlados y con vocabularios gen√©ricos, empezaron a surgir multitud de dificultades. Entre ellas, pueden mencionarse por ejemplo los problemas de polisemia y sinonimia.

En los √∫ltimos a√±os, las aportaciones que se han hecho desde este dominio han mejorado sustancialmente, permitiendo el procesamiento de ingentes cantidades de informaci√≥n en formato texto con un grado de eficacia aceptable. Muestra de ello es la aplicaci√≥n de estas t√©cnicas como una componente esencial en los **MOTORES DE B√öSQUEDA WEB**, en las herramientas de traducci√≥n autom√°tica, o en la generaci√≥n autom√°tica de res√∫menes._



## 2.	EL PROCESAMIENTO DEL LENGUAJE NATURAL EN LA RECUPERACI√ìN DE INFORMACI√ìN TEXTUAL üöÄ

_La complejidad asociada al lenguaje natural cobra especial relevancia cuando necesitamos recuperar informaci√≥n textual que satisfaga la necesidad de informaci√≥n de un usuario. Es por ello, que en el √°rea de Recuperaci√≥n de Informaci√≥n Textual las t√©cnicas de NLP son muy utilizadas, tanto para facilitar la descripci√≥n del contenido de los documentos, como para representar la consulta formulada por el usuario, y ello, con el objetivo de comparar ambas descripciones y presentar al usuario aquellos documentos que satisfagan en mayor grado su necesidad de informaci√≥n._

_Un sistema de recuperaci√≥n de informaci√≥n textual lleva a cabo las siguientes tareas para responder a las consultas de un usuario (imagen 1):_

* 1.	Indexaci√≥n de la colecci√≥n de documentos: en esta fase, mediante la aplicaci√≥n de t√©cnicas de NLP, se genera un √≠ndice que contiene las descripciones de los documentos. Normalmente, cada documento es descrito mediante el conjunto de t√©rminos que, hipot√©ticamente, mejor representa su contenido.
* 2.	Cuando un usuario formula una consulta el sistema la analiza, y si es necesario la transforma, con el fin de representar la necesidad de informaci√≥n del usuario del mismo modo que el contenido de los documentos.
* 3.	El sistema compara la descripci√≥n de cada documento con la descripci√≥n de la consulta, y presenta al usuario aquellos documentos cuyas descripciones m√°s se asemejan a la descripci√≥n de su consulta.
* 4.	Los resultados suelen ser mostrados en funci√≥n de su relevancia, es decir, ordenados en funci√≥n del grado de similitud entre las descripciones de los documentos y de la consulta.

```
Imagen 1: Arquitectura de un sistema de recuperaci√≥n de informaci√≥n.
```

### 2.1.	Procesamiento estad√≠stico del lenguaje natural üì¶

El procesamiento estad√≠stico del lenguaje natural  representa el modelo cl√°sico de los sistemas de recuperaci√≥n de informaci√≥n, y se caracteriza porque cada documento est√° descrito por un conjunto de palabras clave denominadas t√©rminos √≠ndice.

Este enfoque es muy simple, y se basa en lo que se ha denominado como "bolsa de palabras" (o "bag of words"). En esta aproximaci√≥n, todas las palabras de un documento se tratan como t√©rminos √≠ndices para ese documento. Adem√°s se asigna un peso a cada t√©rmino en funci√≥n de su importancia, determinada normalmente por su frecuencia de aparici√≥n en el documento. De este modo, no se toma en consideraci√≥n el orden, la estructura, el significado, etc. de las palabras.

Estos modelos se limitan, por tanto, a emparejar las palabras en los documentos con las palabras en las consultas. Su simplicidad y eficacia los han convertido hoy en los modelos m√°s utilizados en los sistemas de recuperaci√≥n de informaci√≥n textual.

En este modelo el procesamiento de los documentos consta de las siguientes etapas:

* 1.	Pre-procesado de los documentos: consiste fundamentalmente en preparar los documentos para su parametrizaci√≥n, eliminando aquellos elementos que se consideran superfluos.

* 2.	Parametrizaci√≥n: es una etapa de complejidad m√≠nima una vez se han identificado los t√©rminos relevantes. Consiste en realizar una cuantificaci√≥n de las caracter√≠sticas (es decir, de los t√©rminos) de los documentos.
Vamos a ilustrar su funcionamiento mediante el uso del primer p√°rrafo de este mismo documento, suponiendo que √©ste est√° etiquetado en XML. As√≠, el documento sobre el que aplicar√≠amos las t√©cnicas de pre-procesado y parametrizaci√≥n ser√≠a el siguiente:

```
Ejemplo 2. Documento HTML
```

#### A.	El pre-procesado de los documentos: 
consta de tres fases b√°sicas:

1. Eliminaci√≥n de los elementos del documento que no son objeto de indexaci√≥n (o stripping), como podr√≠an ser ciertas etiquetas o cabeceras de los documentos (ejemplo 3).

```
Ejemplo 3. Documento sin cabeceras ni etiquetas
```
2. Normalizaci√≥n de textos, que consiste en homogeneizar todo el texto de la colecci√≥n de documentos sobre la que se trabajar√°, y que afecta por ejemplo a la consideraci√≥n de los t√©rminos en may√∫scula o min√∫scula; el control de determinados par√°metros como cantidades num√©ricas o fechas; el control de abreviaturas y acr√≥nimos, eliminaci√≥n de palabras vac√≠as mediante la aplicaci√≥n de listas de palabras funci√≥n (preposiciones, art√≠culos, etc.), la identificaci√≥n de N-Gramas (los t√©rminos compuestos, subrayados en el ejemplo), etc. (Ejemplo 4).

```
Ejemplo 4. Documento normalizado
```
3. Lematizaci√≥n de los t√©rminos, que es una parte del procesamiento ling√º√≠stico que trata de determinar el lema de cada palabra que aparece en un texto. Su objetivo es reducir una palabra a su ra√≠z, de modo que las palabras clave de una consulta o documento se representen por sus ra√≠ces en lugar de por las palabras originales. El lema de una palabra comprende su forma b√°sica m√°s sus formas declinadas. Por ejemplo, "informa" podr√≠a ser el lema de "informaci√≥n", "informaciones", e "informar". El proceso de lematizaci√≥n (ejemplo 5) se lleva a cabo utilizando algoritmos de radicaci√≥n (o stemming), que permiten representar de un mismo modo las distintas variantes de un t√©rmino, a la vez que reducen el tama√±o del vocabulario y mejoran, en consecuencia, la capacidad de almacenamiento de los sistemas y el tiempo de procesamiento de los documentos. No obstante, estos algoritmos presentan el inconveniente de no agrupar en ocasiones palabras que deber√≠an estarlo, y viceversa, mostrar como iguales palabras que realmente son distintas.

```
Ejemplo 5. Documento con t√©rminos lematizados.
```
Por √∫ltimo, y aunque se han mencionado de pasada, es necesario describir dos t√©cnicas muy utilizadas en el procesamiento estad√≠stico del lenguaje natural, a saber:
*	**La detecci√≥n de N-Gramas:** consiste en la identificaci√≥n de aquellas palabras que suelen aparecer juntas (palabras compuestas, nombres propios, etc.) con el fin de tratarlas como una sola unidad conceptual. Suele hacerse estimando la probabilidad de que dos palabras que aparecen con cierta frecuencia juntas constituyan realmente un solo t√©rmino (compuesto). Estas t√©cnicas tratan de identificar t√©rminos compuestos tales como "accomodation service" o "European Union".

*	**Listas de palabras vac√≠as o palabras funci√≥n (stopwords lists):** una lista de palabras vac√≠as es un listado de t√©rminos (preposiciones, determinantes, pronombres, etc.) considerados de escaso valor sem√°ntico, que cuando se identifican en un documento se eliminan, sin considerarse t√©rminos √≠ndices para la colecci√≥n de textos a analizar. La supresi√≥n de todos estos t√©rminos evita los problemas de ruido documental y supone un considerable ahorro de recursos, ya que aunque se trata de un n√∫mero relativamente reducido de elementos tienen una elevada tasa de frecuencia en los documentos.

#### B.	La parametrizaci√≥n de los documentos: 
Consiste en asignar un peso a cada uno de los t√©rminos relevantes asociados a un documento. El peso de un t√©rmino se calcula normalmente en funci√≥n de su frecuencia de aparici√≥n en el documento, e indica la importancia de dicho t√©rmino como descriptor del contenido de ese documento (ejemplo 6).

```
Ejemplo 6. Fragmento de un documento parametrizado (n√≥tese que las frecuencias asociadas a cada t√©rmino cambiar√≠an a medida que se avanzara en la cuantificaci√≥n de los restantes t√©rminos del documento).
```
Uno de los m√©todos m√°s utilizados para estimar la importancia de un t√©rmino es el conocido sistema TF.IDF (Term Frecuency, Inverse Document Frecuency). Est√° pensado para calcular la importancia de un t√©rmino en funci√≥n de su frecuencia de aparici√≥n en un documento, pero supeditado a su frecuencia de aparici√≥n total en el conjunto de documentos de la colecci√≥n. Es decir, el hecho de que un t√©rmino aparezca muchas veces en un documento es indicativo de que ese t√©rmino es representativo del contenido del mismo, pero siempre y cuando este t√©rmino no aparezca con una frecuencia muy alta en todos los documentos. De ser as√≠, no tendr√≠a ning√∫n valor discriminatorio (por ejemplo, en una base de datos de recetas no tendr√≠a ning√∫n sentido representar el contenido de un documento por la palabra alimento, por m√°s veces que √©sta aparezca).

```
Imagen 7: Frecuencia de T√©rmino (FT)
(A mayor n√∫mero de operaciones mayor score)
```
```
Imagen 8: Frecuencia inversa de documento (IDF)
(Mayor ser√° el score cuanto menos frecuente sea el t√©rmino a lo largo de los documentos)
```
TF-IDF, es la combinaci√≥n de la frecuencia de t√©rmino y la frecuencia inversa de documento, esto permite definir un peso para cada t√©rmino en cada documento.
```
Imagen 9: Formula TF-IDF
```
Caracter√≠sticas:
*	Alto para aquellos t√©rminos que aparecen muchas veces en pocos documentos.
*	Bajo para aquellos t√©rminos que aparezcan pocas veces o en muchos documentos.
*	Bajo para aquellos t√©rminos que aparezcan en pr√°cticamente todos los documentos (stop words)

### 2.2.	Modelo espacio vectorial

*	Al trabajar con vectores se puede calcular el producto escalar entre los documentos.
*	El conjunto de documentos debe verse como un conjunto de vectores en un espacio vectorial donde cada eje ser√° cada t√©rmino

```
Imagen 10: Similitud Coseno
```
Para poder calcular el espacio vectorial, necesitamos de la matr√≠z de incidencia.(Imagen 11) Dicha estructura podr√≠a ser alimentada con los valores obtenidos con el sistema TF-IDF  o por coincidencias. Como se puede notar es un cruce de informaci√≥n entre el corpus y las palabras tokenizadas.

```
Imagen 11: Similitud Coseno
```
Al trabajar con vectores se puede calcular el producto escalar entre los documentos y es √∫til para comparar documentos

```
Imagen 12: Similitud Coseno
```
## 3.	ESTRUCTURA DEL PROYECTO

### 3.1.	DATA
*	___matrizdispercion100000_0.npz: Almacena la matriz de dispersi√≥n
*	___matrizdispercionInv100000_0.csv: Almacena la matriz de dispersi√≥n inversa.
*	big.txt: Almacena un libro que utiliza como vocabulario para corregir las cadenas buscadas.
*	DFdatalWiki_completo100000.dat: Almacena las p√°ginas web completas.
*	DFdatalWiki_limpio100000.csv: Almacena la data limpia de ruido.
*	wikipedia_es_all_nopic_2020-09.zim: Es el dataset de Wikipedia con todas las p√°ginas web.

### 3.2.	TEMPLATES
*	base.html
*	home.html
*	inicio.html
*	resIndividual.html

### 3.3.	TRATAMIENTO
*	Zimscan: Librer√≠a para leer el dataset de wikipedia
*	autocorrector.py: Clase para corregir los Querys buscados.
*	depurador.py: Clase para eliminar el ruido del dataset
*	generarmatrizDispercion.py: Algoritmo para poder generar y almacenar la matriz de dispersi√≥n.
*	trabajarData.py: Algoritmo para guardar el dataset de Wikipedia en un formato .dat.

### 3.4.	RAIZ: 
*	controlador.py: Clase que inicializa las rutas de acceso a los datos
  *	buscar: Funci√≥n que permite buscar el Query.
  *	vectorizarQuery: Funci√≥n que permite vectorizar el query.
  *	getPagina: Funci√≥n que permite ubicar el c√≥digo de una p√°gina seg√∫n el √≠ndice.
*	index.py: Hoja para routear las urls seg√∫n el framework flask.

## 4.	ESTRUCTURA DE DATOSüöÄ

Para el proyecto se hizo un an√°lisis de rendimiento para determinar la estructura adecuada para la recuperaci√≥n de datos. Como prueba se realiz√≥ una b√∫squeda en 100000 registros y se obtuvo los siguientes resultados:

### 1.1.	DataFrame: Los dataframes son una clase de objetos especial donde cada fila corresponde a un objeto de la muestra y cada columna a una variable. Un dataframe es muy similar a la de una matriz. Pero en una matriz solamente se admiten valores num√©ricos, a diferencia de la matriz, en un dataframe se puede incluir tambi√©n datos alfanum√©ricos en su contenido.
 
```
	Tiempo de ejecuci√≥n: 53.968310832977295 segundos.
```
### 1.2.	Zim: El formato de archivo ZIM es un formato de archivo abierto que almacena contenido wiki para su uso sin conexi√≥n. El formato permite la compresi√≥n de art√≠culos, presenta un √≠ndice de b√∫squeda de texto completo y un manejo nativo de categor√≠as e im√°genes similar a MediaWiki, y todo el archivo se puede indexar y leer f√°cilmente con un programa como Kiwix, a diferencia de los volcados de bases de datos XML nativos de Wikipedia.

```
	Tiempo de ejecuci√≥n: 31.599377870559692 segundos.
```
### 1.3.	Nemmap: Es un mapa de memoria para una matriz almacenada en un archivo binario en el disco. Los archivos asignados en memoria se utilizan para acceder a peque√±os segmentos de archivos grandes en el disco, sin leer el archivo completo en la memoria. Los memmap de Numpy son objetos en forma de matriz. Esto difiere del m√≥dulo mmap de Python, que usa objetos similares a archivos.

```  
	Tiempo de ejecuci√≥n: 0.04995560646057129 segundos.
```

Se puede notar despu√©s de la evaluaci√≥n que el formato m√°s adecuado para la recuperaci√≥n de informaci√≥n es Nemmap; la dificultad que presenta, es el espacio de memoria que ocupa el archivo, a diferencia de un dataframe que es mucho menor, pero el tiempo de recuperaci√≥n es mayor.
Para el almacenamiento de la matriz dispersa, utilizamos los Matrix Sparse, porque despu√©s de leer varios post notamos que es utilizado frecuentemente para estos trabajos. 
 
Fragmento de c√≥digo para guardar un archivo sparce matrix.
Y para el almacenamiento de la matriz inversa con el que podemos vectorizar la consulta utilizamos los dataframe ya que nos ofrece un formato ligero y nos permite manejar cabeceras. Tambi√©n lo utilizamos para almacenar la data tratada, sin ruido, preprocesada, que nos servir√° para generar la matriz dispersa.


## 5. SUGERENCIASüöÄ

* Para un proceso de b√∫squeda m√°s r√°pido se sugiere generar la matriz de incidencia antes de correr el programa. Es decir almacenarlo en un dataframe o matriz sparse en forma inversa.

*	Para alivianar el peso de la matriz de incidencia debemos almacenarla como una matriz inversa dejando los 0s de lado.

*	Existen funciones que deber√≠an revisarse, como el proceso de lematizaci√≥n. Porque estas funciones utilizadas est√°n escritas por personas de habla inglesa; a pesar de que tiene la opci√≥n de ponerlo en espa√±ol se nota que no manipula bien las palabras del idioma.

*	Para mejorar la salida del sistema se deber√≠a de involucrar al usuario (feedback). 

```
Feedback expl√≠cito: Entrenar al sistema de la siguiente forma:

1.	Un usuario realiza una consulta
2.	El sistema devuelve un conjunto de resultados
3.	El usuario anota algunos resultados como relevantes y no relevantes
4.	El sistema recalcula una mejor representaci√≥n de la informaci√≥n
5.	El sistema devuelve una versi√≥n revisada de los resultados.

Feedback impl√≠cito: En este caso la relevancia de los documentos se infiere en base al comportamiento de los usuarios:
‚Ä¢	Qu√© documentos se seleccionan y cu√°les no
‚Ä¢	Acciones en la b√∫squeda y scrolling
‚Ä¢	Duraci√≥n del tiempo que un usuario pasa leyendo un documento (Dwell time)
```

*	Para expansi√≥n de consultas se sugiere involucrar tareas de b√∫squeda de sin√≥nimos y correcci√≥n de faltas ortogr√°ficas.

*	Por √∫ltimo hace falta aplicar algunas t√©cnicas m√°s de NLP para mejorar el los sistemas de recuperaci√≥n:
```
‚Ä¢	Correcci√≥n de errores
‚Ä¢	Detecci√≥n de sin√≥nimos
‚Ä¢	Desambiguaci√≥n ling√º√≠stica
‚Ä¢	Detecci√≥n de t√©rminos relevantes en la consulta
‚Ä¢	Detecci√≥n de la intenci√≥n en la consulta
‚Ä¢	Detenci√≥n de entidades en la consulta
‚Ä¢	Clasificaci√≥n de documentos.
```

## 4. LINKS üéÅ

*	https://es.wikipedia.org/wiki/Wikipedia:Descargas
*	https://pypi.org/project/zimscan/
*	https://www.upf.edu/hipertextnet/numero-5/pln.html
*	https://www.notion.so/NLP-Natural-Language-Processing-32a706bbb0b64481ab597146e8b7abdc
*	https://uniwebsidad.com/libros/python/capitulo-6/metodos-de-busqueda
*	https://jordycuan.github.io/2016/03/27/indexado-1/
*	https://jarroba.com/scraping-python-beautifulsoup-ejemplos/
*	https://medium.com/datos-y-ciencia/preprocesamiento-de-datos-de-texto-un-tutorial-en-python-5db5620f1767
*	https://code.tutsplus.com/es/tutorials/scraping-webpages-in-python-with-beautiful-soup-search-and-dom-modification--cms-28276
*	https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
*	https://www.w3resource.com/numpy/input-and-output/memmap.php
*	https://www.diegocalvo.es/dataframes-in-python/
*	https://www.geeksforgeeks.org/saving-a-pandas-dataframe-as-a-csv/
*	https://www.researchgate.net/post/How_to_append_TF-IDF_vector_into_pandas_dataframe
*	https://stackoverflow.com/questions/44193154/notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted
*	http://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/
*	https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a
*	https://medium.com/@hdezfloresmiguelangel/implementando-un-corrector-ortogr%C3%A1fico-en-python-utilizando-la-distancia-de-levenshtein-498ec0dd1105
*	https://flask.palletsprojects.com/en/1.1.x/
*	https://es.wikipedia.org/wiki/Wikipedia:Descargas

